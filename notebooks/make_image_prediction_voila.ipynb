{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# People Counter\n",
    "\n",
    "This website is a sample version of the Public Eye system. You can upload a picture, and count our People Counter will count the number op people on this picture.\n",
    "\n",
    "The processing speed is much slower than the actual Public Eye system. However, the counting performance is usually quite good.\n",
    "\n",
    "For more information about Public Eye, please get in touch with us at ADD_EMAIL_HERE. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-16T15:37:48.721180Z",
     "start_time": "2022-08-16T15:37:48.715298Z"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-16T15:37:51.215669Z",
     "start_time": "2022-08-16T15:37:48.722870Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "\n",
    "import models.ViCCT_models\n",
    "import models.Swin_ViCCT_models\n",
    "from timm.models import create_model\n",
    "\n",
    "from datasets.dataset_utils import img_equal_split, img_equal_unsplit\n",
    "import torchvision.transforms as standard_transforms\n",
    "\n",
    "from fastai.vision.widgets import *\n",
    "from fastbook import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-16T15:37:51.219673Z",
     "start_time": "2022-08-16T15:37:51.217256Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set some parameters\n",
    "# Several parameters need to be defined to run this notebook. The cell below is the only cell that needs modification in this notebook.\n",
    "\n",
    "# First, which model will we use?\n",
    "# The generic ViCCT version 1 model is specified with 'ViCCT_base'. \n",
    "# The version 2 ViCCT model, which has Swin as its base, is specified with 'Swin_ViCCT_large_22k'.\n",
    "# model_name = 'ViCCT_base'\n",
    "model_name = 'Swin_ViCCT_large_22k'\n",
    "\n",
    "# The model is trained to perform crowd counting. We specify here where the weights of this trained model is located.\n",
    "# weights_path = 'models/trained_models/ViCCT_base_generic_1300_epochs.pth'\n",
    "weights_path = 'models/trained_models/Swin_ViCCT_large_22k_generic_1600_epochs.pth'\n",
    "\n",
    "# Now, for which image will the model make a prediction? We now specify where the image is located.\n",
    "# image_path = '/PATH/TO/YOUR/IMAGE/FOLDER/image.png'\n",
    "\n",
    "# Some images are of extremely large resolution. When the heads in images occupy many (e.g. something like 100 x 100 \n",
    "# pixels each) pixels, the model is unable to make pretty predictions. One way to overcome this issue is to scale the image\n",
    "# by some factor. This factory is specified here. A factor of 1. means no scaling is performed.\n",
    "# scale_factor = 1.\n",
    "scale_factor = 0.2\n",
    "\n",
    "# We might want to save the predictions. Set 'save_results' to true if you want to save the prediction. Three figures are saved\n",
    "# 1) The input image for the network. 2) The network's prediction. 3) The predictions overlayed with the input.\n",
    "save_results = True\n",
    "\n",
    "# Lastly, do we use cuda? If you have cuda, it's advised to use it.\n",
    "use_cuda = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-16T15:37:53.282196Z",
     "start_time": "2022-08-16T15:37:51.221224Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load model\n",
    "def load_model(model_name, weights_path, use_cuda):\n",
    "    \"\"\" Creates the model and initialised it with the weights specified. \"\"\"\n",
    "    \n",
    "    model = create_model(  # From the timm library. This function created the model specific architecture.\n",
    "    model_name,\n",
    "    init_path=weights_path,\n",
    "    pretrained_cc=True,\n",
    "    drop_rate=None if 'Swin' in model_name else 0.,  # Dropout\n",
    "\n",
    "    # Bamboozled by Facebook. This isn't drop_path_rate, but rather 'drop_connect'.\n",
    "    # I'm not yet sure what it is for the Swin version\n",
    "    drop_path_rate=None if 'Swin' in model_name else 0.,\n",
    "    drop_block_rate=None,  # Drops our entire Transformer blocks I think? Not used for ViCCT.\n",
    "    )\n",
    "\n",
    "    if use_cuda:\n",
    "        model = model.cuda()  # Place model on GPU\n",
    "    \n",
    "    model = model.eval()\n",
    "    \n",
    "    return model\n",
    "\n",
    "# print('Loading the model...')\n",
    "model = load_model(model_name, weights_path, use_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-16T15:37:53.347813Z",
     "start_time": "2022-08-16T15:37:53.283478Z"
    }
   },
   "outputs": [],
   "source": [
    "# Main Functions\n",
    "\n",
    "\n",
    "# Only for hardcore users. No need to modify these.\n",
    "mean_std = ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # Mean and std.dev. of ImageNet\n",
    "overlap = 32  # We ensure crops have at least this many pixels of overlap.\n",
    "ignore_buffer = 16  # When reconsturting the whole density map, ignore this many pixels on crop prediction borders.\n",
    "\n",
    "train_img_transform = standard_transforms.Compose([\n",
    "    standard_transforms.ToTensor(),\n",
    "    standard_transforms.Normalize(*mean_std)\n",
    "])\n",
    "\n",
    "\n",
    "def rescale_image(img, scale_factor):\n",
    "    \n",
    "    # Get image dimensions\n",
    "    img_w, img_h = img.size\n",
    "    \n",
    "    # Rescale image\n",
    "    if scale_factor != 1.:\n",
    "        new_w, new_h = round(img_w * scale_factor), round(img_h * scale_factor)\n",
    "        img = img.resize((new_w, new_h))\n",
    "    \n",
    "    return img\n",
    "\n",
    "\n",
    "def prepare_loaded_image(img):\n",
    "    \n",
    "    # Get image dimensions\n",
    "    img_w, img_h = img.size\n",
    "    \n",
    "    # Before we make the prediction, we normalise the image and split it up into crops\n",
    "    # print('Preparing image...')\n",
    "    img = train_img_transform(img)\n",
    "    img_stack = img_equal_split(img, 224, overlap)  # Split the image ensuring a minimum of 'overlap' of overlap between crops.\n",
    "\n",
    "    if use_cuda:\n",
    "        img_stack = img_stack.cuda()  # Place image stack on GPU        \n",
    "\n",
    "    # This is the placeholder where we store the model predictions.\n",
    "    pred_stack = torch.zeros(img_stack.shape[0], 1, 224, 224)\n",
    "    \n",
    "    return img_stack, pred_stack, img_h, img_w\n",
    "\n",
    "\n",
    "def process_image(img_stack, pred_stack, img_h, img_w):\n",
    "    # print('Making prediction now...')\n",
    "    if not use_cuda and img_stack.shape[0] > 100:  # If on CPU and more than 100 image crops.\n",
    "        print('\\033[93m'\n",
    "              'WARNING: you are making a prediction on the CPU but provided a large image. This might take a'\n",
    "              ' (very) long time! You might want to consider downsizing the image with \"scale_factor\".'\n",
    "              '\\033[0m')\n",
    "\n",
    "    with torch.no_grad():  # Dont make gradients\n",
    "        print(f\"Processing {len(img_stack)} image parts.\")\n",
    "        for idx, img_crop in enumerate(tqdm(img_stack)):  # For each image crop\n",
    "        # for idx, img_crop in enumerate(img_stack):  # For each image crop\n",
    "        #     print(f'Processing part {idx} of {img_stack.shape[0]}')\n",
    "            pred_stack[idx] = model.forward(img_crop.unsqueeze(0)).cpu()  # Make prediction.\n",
    "    print('Done!')\n",
    "\n",
    "\n",
    "    # Unsplit the perdiction crops to get the entire density map of the image.\n",
    "    den = img_equal_unsplit(pred_stack, overlap, ignore_buffer, img_h, img_w, 1)\n",
    "    den = den.squeeze()  # Remove the channel dimension\n",
    "\n",
    "    # Compute the perdicted count, which is the sum of the entire density map. Note that the model is trained with density maps\n",
    "    # scaled by a factor of 3000 (See sec 5.2 of my thesis for why: https://scripties.uba.uva.nl/search?id=723178). In short,\n",
    "    # This works :)\n",
    "    pred_cnt = den.sum() / 3000\n",
    "    \n",
    "    return den, pred_cnt\n",
    "\n",
    "\n",
    "def show_overlay(input_image, den, pred_cnt):\n",
    "    img_heat = np.array(input_image)\n",
    "    den_heat = den.clone().numpy()\n",
    "\n",
    "    den_heat = den_heat / 3000  # Scale values to original domain\n",
    "    den_heat[den_heat < 0] = 0  # Remove negative values\n",
    "    den_heat = den_heat / den_heat.max() # Normalise between 0 and 1\n",
    "\n",
    "    den_heat **= 0.5  # Reduce large values, increase small values\n",
    "    den_heat *= 255  # Values from 0 to 255 now\n",
    "    den_heat[den_heat < 50] = 0  # Threshold of 50\n",
    "\n",
    "    img_heat[:, :, 0][den_heat > 0] = img_heat[:, :, 0][den_heat > 0] / 2\n",
    "    img_heat[:, :, 1][den_heat > 0] = img_heat[:, :, 1][den_heat > 0] / 2\n",
    "    img_heat[:, :, 2][den_heat > 0] = den_heat[den_heat > 0]\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(1920/200, 1080/200), dpi=200)\n",
    "    plt.imshow(img_heat)\n",
    "    plt.title(f'[OVERLAY] Predicted count: {pred_cnt:.3f}')\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def save_result_func(image_path, input_image, prediction, predicted_count, overlayed):\n",
    "    \"\"\" Saves the results in the 'try_image_result' directory. Makes the directory if it doesn't exist. \"\"\"\n",
    "    \n",
    "    # We save results here\n",
    "    save_dir = 'try_image_result'\n",
    "    \n",
    "    # Make dir if not exists\n",
    "    if not os.path.exists(save_dir):  \n",
    "        os.mkdir(save_dir)\n",
    "        \n",
    "    # Extract just the filename from image path\n",
    "    full_name = os.path.basename(image_path)\n",
    "    file_name = os.path.splitext(full_name)[0]\n",
    "    \n",
    "    # The paths where to save the results\n",
    "    input_save_name = file_name + '_input' + '.jpg'\n",
    "    img_save_path = os.path.join(save_dir, input_save_name)\n",
    "    predicion_save_name = file_name + '_prediction' + '.jpg'\n",
    "    pred_save_path = os.path.join(save_dir, predicion_save_name)\n",
    "    overlayed_save_name = file_name + '_overlayed' + '.jpg'\n",
    "    overlayed_save_path = os.path.join(save_dir, overlayed_save_name)\n",
    "\n",
    "    \n",
    "    # Save results\n",
    "    fig = plt.figure(figsize=(1920/200, 1080/200), dpi=200)\n",
    "    plt.imshow(input_image, cmap=cm.jet)\n",
    "    plt.title(f'Input image for the network')\n",
    "    plt.savefig(img_save_path, dpi=200, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    \n",
    "    fig = plt.figure(figsize=(1920/200, 1080/200), dpi=200)\n",
    "    plt.imshow(prediction, cmap=cm.jet)\n",
    "    plt.title(f'Predicted count: {predicted_count:.3f}')\n",
    "    plt.savefig(pred_save_path, dpi=200, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    \n",
    "    fig = plt.figure(figsize=(1920/200, 1080/200), dpi=200)\n",
    "    plt.imshow(overlayed, cmap=cm.jet)\n",
    "    plt.title(f'Predicted count: {predicted_count:.3f}')\n",
    "    plt.savefig(overlayed_save_path, dpi=200, bbox_inches='tight')\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-16T15:37:53.371243Z",
     "start_time": "2022-08-16T15:37:53.349251Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create widget items.\n",
    "btn_upload = widgets.FileUpload()\n",
    "scale_factor_slider = widgets.FloatSlider(value=0.5, min=0.05, max=1, step=0.05, description=\"Downscale\")\n",
    "out_pl = widgets.Output()\n",
    "lbl_pred = widgets.Label()\n",
    "btn_run = widgets.Button(description='Count People')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-16T15:37:53.378755Z",
     "start_time": "2022-08-16T15:37:53.372828Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define click interaction.\n",
    "def on_click_classify(change):\n",
    "#     input_image = Image.open(io.BytesIO(btn_upload.data[-1]))\n",
    "    input_image = PILImage.create(io.BytesIO(btn_upload.data[-1]))\n",
    "    out_pl.clear_output()\n",
    "    with out_pl:\n",
    "        \n",
    "        # Show input image\n",
    "        #display(img.to_thumb(128,128))\n",
    "        plt.figure(figsize=(1920/200, 1080/200), dpi=200)\n",
    "        plt.imshow(input_image, cmap=cm.jet)\n",
    "        plt.title(f'Input image for the network')\n",
    "        plt.show()\n",
    "        \n",
    "        # Scale image\n",
    "        scale_factor = scale_factor_slider.value\n",
    "        print(f\"Downscale factor used for processing: {scale_factor}\")\n",
    "        image = rescale_image(input_image, scale_factor)\n",
    "        \n",
    "        # Process image\n",
    "        img_stack, pred_stack, img_h, img_w = prepare_loaded_image(image)\n",
    "        den, pred_cnt = process_image(img_stack, pred_stack, img_h, img_w)\n",
    "        \n",
    "        # Show model prediction\n",
    "        plt.figure(figsize=(1920/200, 1080/200), dpi=200)\n",
    "        plt.imshow(den, cmap=cm.jet)\n",
    "        plt.title(f'[DENSITY MAP] Predicted count: {pred_cnt:.3f}')\n",
    "        plt.show()\n",
    "        \n",
    "        # Show overlay\n",
    "        show_overlay(image, den, pred_cnt)\n",
    "\n",
    "btn_run.on_click(on_click_classify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-16T15:37:53.398335Z",
     "start_time": "2022-08-16T15:37:53.381459Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Render app.\n",
    "# VBox([widgets.Label(\"Choose and image to upload\"),\n",
    "VBox([widgets.HTML(value = f\"<b><font color='red'>Choose an image to upload:</b>\"),\n",
    "      btn_upload, scale_factor_slider, btn_run, out_pl, lbl_pred])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
