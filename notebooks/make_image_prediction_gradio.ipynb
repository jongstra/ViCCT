{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Public Eye - Sample Version\n",
    "\n",
    "**Upload a picture of a crowd and let Public Eye count/estimate the amount of people!**\n",
    "\n",
    "This website is a slow and minimal sample version of the full Public Eye system.\n",
    "\n",
    "For more information about Public Eye, please get in touch at [crowdcounter@amsterdam.nl](mailto:crowdcounter@amsterdam.nl)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-14T13:33:29.167516Z",
     "start_time": "2022-10-14T13:33:29.142114Z"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-14T13:33:32.469107Z",
     "start_time": "2022-10-14T13:33:29.266151Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "\n",
    "import models.ViCCT_models\n",
    "import models.Swin_ViCCT_models\n",
    "from timm.models import create_model\n",
    "\n",
    "from datasets.dataset_utils import img_equal_split, img_equal_unsplit\n",
    "import torchvision.transforms as standard_transforms\n",
    "\n",
    "from fastai.vision.widgets import *\n",
    "from fastbook import *\n",
    "\n",
    "# from pyngrok import ngrok\n",
    "\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-14T13:33:32.473132Z",
     "start_time": "2022-10-14T13:33:32.470739Z"
    }
   },
   "outputs": [],
   "source": [
    "# # https://towardsdatascience.com/quickly-share-ml-webapps-from-google-colab-using-ngrok-for-free-ae899ca2661a\n",
    "# # Open a HTTP tunnel on the default port 80\n",
    "# public_url = ngrok.connect(port = '8888')\n",
    "# print(public_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-14T13:33:32.477801Z",
     "start_time": "2022-10-14T13:33:32.474312Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set some parameters\n",
    "# Several parameters need to be defined to run this notebook.\n",
    "\n",
    "# First, which model will we use?\n",
    "# The generic ViCCT version 1 model is specified with 'ViCCT_base'. \n",
    "# The version 2 ViCCT model, which has Swin as its base, is specified with 'Swin_ViCCT_large_22k'.\n",
    "# model_name = 'ViCCT_base'\n",
    "model_name = 'Swin_ViCCT_large_22k'\n",
    "\n",
    "# The model is trained to perform crowd counting. We specify here where the weights of this trained model is located.\n",
    "# weights_path = 'models/trained_models/ViCCT_base_generic_1300_epochs.pth'\n",
    "weights_path = 'models/trained_models/Swin_ViCCT_large_22k_generic_1600_epochs.pth'\n",
    "\n",
    "# Some images are of extremely large resolution. When the heads in images occupy many (e.g. something like 100 x 100 \n",
    "# pixels each) pixels, the model is unable to make pretty predictions. One way to overcome this issue is to scale the image\n",
    "# by some factor. This factory is specified here. A factor of 1. means no scaling is performed.\n",
    "# scale_factor = 1.\n",
    "\n",
    "# We might want to save the predictions. Set 'save_results' to true if you want to save the prediction. Three figures are saved\n",
    "# 1) The input image for the network. 2) The network's prediction. 3) The predictions overlayed with the input.\n",
    "save_results = True\n",
    "\n",
    "# Lastly, do we use cuda? If you have cuda, it's advised to use it.\n",
    "use_cuda = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-14T13:33:34.444344Z",
     "start_time": "2022-10-14T13:33:32.480918Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jongstra/.local/lib/python3.10/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2895.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "def load_model(model_name, weights_path, use_cuda):\n",
    "    \"\"\" Creates the model and initialised it with the weights specified. \"\"\"\n",
    "    \n",
    "    model = create_model(  # From the timm library. This function created the model specific architecture.\n",
    "    model_name,\n",
    "    init_path=weights_path,\n",
    "    pretrained_cc=True,\n",
    "    drop_rate=None if 'Swin' in model_name else 0.,  # Dropout\n",
    "\n",
    "    # Bamboozled by Facebook. This isn't drop_path_rate, but rather 'drop_connect'.\n",
    "    # I'm not yet sure what it is for the Swin version\n",
    "    drop_path_rate=None if 'Swin' in model_name else 0.,\n",
    "    drop_block_rate=None,  # Drops our entire Transformer blocks I think? Not used for ViCCT.\n",
    "    )\n",
    "\n",
    "    if use_cuda:\n",
    "        model = model.cuda()  # Place model on GPU\n",
    "    \n",
    "    model = model.eval()\n",
    "    \n",
    "    return model\n",
    "\n",
    "# print('Loading the model...')\n",
    "model = load_model(model_name, weights_path, use_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-14T13:33:34.454310Z",
     "start_time": "2022-10-14T13:33:34.445742Z"
    }
   },
   "outputs": [],
   "source": [
    "# Main Functions\n",
    "\n",
    "\n",
    "# Only for hardcore users. No need to modify these.\n",
    "mean_std = ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # Mean and std.dev. of ImageNet\n",
    "overlap = 32  # We ensure crops have at least this many pixels of overlap.\n",
    "ignore_buffer = 16  # When reconsturting the whole density map, ignore this many pixels on crop prediction borders.\n",
    "\n",
    "train_img_transform = standard_transforms.Compose([\n",
    "    standard_transforms.ToTensor(),\n",
    "    standard_transforms.Normalize(*mean_std)\n",
    "])\n",
    "\n",
    "\n",
    "def rescale_image(img, scale_factor):\n",
    "    # Get image dimensions\n",
    "    img_w, img_h = img.size\n",
    "    \n",
    "    # Rescale image\n",
    "    if scale_factor != 1.:\n",
    "        new_w, new_h = round(img_w * scale_factor), round(img_h * scale_factor)\n",
    "        img = img.resize((new_w, new_h))\n",
    "    \n",
    "    return img\n",
    "\n",
    "\n",
    "def prepare_loaded_image(img):\n",
    "    # Get image dimensions\n",
    "    img_w, img_h = img.size\n",
    "    \n",
    "    # Before we make the prediction, we normalise the image and split it up into crops\n",
    "    img = train_img_transform(img)\n",
    "    img_stack = img_equal_split(img, 224, overlap)  # Split the image ensuring a minimum of 'overlap' of overlap between crops.\n",
    "\n",
    "    if use_cuda:\n",
    "        img_stack = img_stack.cuda()  # Place image stack on GPU        \n",
    "\n",
    "    # This is the placeholder where we store the model predictions.\n",
    "    pred_stack = torch.zeros(img_stack.shape[0], 1, 224, 224)\n",
    "    \n",
    "    return img_stack, pred_stack, img_h, img_w\n",
    "\n",
    "\n",
    "def process_image(img_stack, pred_stack, img_h, img_w):\n",
    "    if not use_cuda and img_stack.shape[0] > 100:  # If on CPU and more than 100 image crops.\n",
    "        print('\\033[93m'\n",
    "              'WARNING: you are making a prediction on a very large image. This might take a long time! '\n",
    "              'You may want to use a lower \"Scale Factor\" value for faster processing. '\n",
    "              'You can stop a running process by pressing F5.'\n",
    "              '\\033[0m')\n",
    "\n",
    "    with torch.no_grad():  # Dont make gradients\n",
    "        print(f\"Processing {len(img_stack)} image parts.\")\n",
    "        for idx, img_crop in enumerate(tqdm(img_stack)):  # For each image crop\n",
    "            pred_stack[idx] = model.forward(img_crop.unsqueeze(0)).cpu()  # Make prediction.\n",
    "    print('Done!')\n",
    "\n",
    "\n",
    "    # Unsplit the perdiction crops to get the entire density map of the image.\n",
    "    den = img_equal_unsplit(pred_stack, overlap, ignore_buffer, img_h, img_w, 1)\n",
    "    den = den.squeeze()  # Remove the channel dimension\n",
    "\n",
    "    # Compute the perdicted count, which is the sum of the entire density map. Note that the model is trained with density maps\n",
    "    # scaled by a factor of 3000 (See sec 5.2 of my thesis for why: https://scripties.uba.uva.nl/search?id=723178). In short,\n",
    "    # This works :)\n",
    "    pred_cnt = den.sum() / 3000\n",
    "    \n",
    "    return den, pred_cnt\n",
    "\n",
    "\n",
    "def show_overlay(input_image, den, pred_cnt):\n",
    "    img_heat = np.array(input_image)\n",
    "    den_heat = den.clone().numpy()\n",
    "\n",
    "    den_heat = den_heat / 3000  # Scale values to original domain\n",
    "    den_heat[den_heat < 0] = 0  # Remove negative values\n",
    "    den_heat = den_heat / den_heat.max() # Normalise between 0 and 1\n",
    "\n",
    "    den_heat **= 0.5  # Reduce large values, increase small values\n",
    "    den_heat *= 255  # Values from 0 to 255 now\n",
    "    den_heat[den_heat < 50] = 0  # Threshold of 50\n",
    "\n",
    "    img_heat[:, :, 0][den_heat > 0] = img_heat[:, :, 0][den_heat > 0] / 2\n",
    "    img_heat[:, :, 1][den_heat > 0] = img_heat[:, :, 1][den_heat > 0] / 2\n",
    "    img_heat[:, :, 2][den_heat > 0] = den_heat[den_heat > 0]\n",
    "\n",
    "\n",
    "#     plt.figure(figsize=(1920/200, 1080/200), dpi=200)\n",
    "    plt.figure(figsize=(1440/200, 810/200), dpi=200)\n",
    "    plt.imshow(img_heat)\n",
    "    plt.title(f'Predicted count: {pred_cnt:.3f}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-14T14:04:07.477845Z",
     "start_time": "2022-10-14T14:04:07.472856Z"
    }
   },
   "outputs": [],
   "source": [
    "def show_image(image, bla):\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-14T14:45:02.547779Z",
     "start_time": "2022-10-14T14:45:02.537197Z"
    }
   },
   "outputs": [],
   "source": [
    "def set_scale_factor(image_input):\n",
    "    try:\n",
    "        y_res, x_res, rbg = image_input.shape\n",
    "    except AttributeError:\n",
    "        return 0.5, 0, 0\n",
    "    min_res = min(x_res, y_res)\n",
    "    ideal_min = 1200\n",
    "    if min_res < ideal_min:\n",
    "        factor = 1\n",
    "    if min_res > ideal_min:\n",
    "        factor = round(ideal_min / min_res, 2) + 0.01\n",
    "        if factor > 1:\n",
    "            factor = 1\n",
    "    return factor, x_res, y_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-14T14:45:03.154894Z",
     "start_time": "2022-10-14T14:45:03.146845Z"
    }
   },
   "outputs": [],
   "source": [
    "def show_slider_val(x):\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-14T15:01:35.164994Z",
     "start_time": "2022-10-14T15:01:35.151977Z"
    }
   },
   "outputs": [],
   "source": [
    "def show_downscaled_res_and_processing_time(x_res, y_res, slider_val):\n",
    "    x_res_downscaled = int(x_res*slider_val)\n",
    "    y_res_downscaled = int(y_res*slider_val)\n",
    "    approximate_n_crops = round((x_res_downscaled*y_res_downscaled) / 30000) + 4 # High estimate for larger images.\n",
    "    approximate_processing_time = int(approximate_n_crops / 6.5)  # on fast server.\n",
    "    return x_res_downscaled, y_res_downscaled, approximate_processing_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-14T15:01:37.856651Z",
     "start_time": "2022-10-14T15:01:35.338630Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7923\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7923/\" width=\"900\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(<gradio.routes.App at 0x7f86d0328580>, 'http://127.0.0.1:7923/', None)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO:\n",
    "#\n",
    "# Make count button work (use model, test GPU version as well)\n",
    "# Put \"People Count\" result number under output image?\n",
    "# Put \"Clear Image\" button under input image?\n",
    "# Built a catch when pressing the count_button if the (downscaled) image is of too low resolution.\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import gradio as gr\n",
    "\n",
    "demo = gr.Blocks()\n",
    "\n",
    "def count_people(x):\n",
    "    print(type(x))\n",
    "    return np.fliplr(x)\n",
    "\n",
    "with demo:\n",
    "    gr.Markdown(\"Upload an image and count the amount of people on it.\")\n",
    "    with gr.Row():\n",
    "        image_input = gr.Image()\n",
    "        image_output = gr.Image()\n",
    "    with gr.Row():\n",
    "        x_res = gr.Number(label=\"Original x resolution\")\n",
    "        y_res = gr.Number(label=\"Original y resolution\")\n",
    "    slider = gr.Slider(0.01, 1, step=0.01, value=0.5, label=\"Downscale factor\")\n",
    "    with gr.Row():\n",
    "        x_res_downscaled = gr.Number(label=\"Downscaled x resolution\")\n",
    "        y_res_downscaled = gr.Number(label=\"Downscaled y resolution\")\n",
    "        appr_proc_time = gr.Number(label=\"Approximate processing time (sec)\")\n",
    "    count_button = gr.Button(\"Count People\")\n",
    "    count_result = gr.Number(label=\"People Count\")\n",
    "    \n",
    "    # Interactions\n",
    "    image_input.change(fn=set_scale_factor, inputs=image_input, outputs=[slider, x_res, y_res])\n",
    "    slider.change(fn=show_downscaled_res_and_processing_time, inputs=[x_res, y_res, slider], outputs=[x_res_downscaled, y_res_downscaled, appr_proc_time])\n",
    "#     count_button.click(count_people, inputs=image_input, outputs=image_output)\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-14T14:05:41.879951Z",
     "start_time": "2022-10-14T14:05:41.874855Z"
    }
   },
   "outputs": [],
   "source": [
    "# demo = gr.Interface(\n",
    "#     fn = show_image,\n",
    "#     inputs = [gr.Image(type=\"pil\"), gr.Slider(0.01, 1, step=0.01, value=0.5)],\n",
    "#     outputs = [\"image\"]\n",
    "# )\n",
    "# demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-14T14:04:44.434713Z",
     "start_time": "2022-10-14T14:04:44.425223Z"
    }
   },
   "outputs": [],
   "source": [
    "# def greet(image, temperature):\n",
    "#     salutation = \"Good morning\" if is_morning else \"Good evening\"\n",
    "#     greeting = f\"{salutation} {name}. It is {temperature} degrees today\"\n",
    "#     celsius = (temperature - 32) * 5 / 9\n",
    "#     return greeting, round(celsius, 2)\n",
    "\n",
    "# demo = gr.Interface(\n",
    "#     fn=greet,\n",
    "#     inputs=[\"image\", gr.Slider(0.01, 1, step=0.01, value=0.5)],\n",
    "#     outputs=[\"text\", \"number\"],\n",
    "# )\n",
    "\n",
    "# demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-14T13:59:52.297561Z",
     "start_time": "2022-10-14T13:59:52.293094Z"
    }
   },
   "outputs": [],
   "source": [
    "# with gr.Blocks() as demo:\n",
    "#     image = gr.Image()\n",
    "#     slider = gr.Slider(0.01, 1, step=0.01, value=0.5)\n",
    "#     image.change(set_slider, slider, slider)\n",
    "    \n",
    "# demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-14T13:45:30.298211Z",
     "start_time": "2022-10-14T13:45:30.291168Z"
    }
   },
   "outputs": [],
   "source": [
    "# demo = gr.Interface(\n",
    "#     fn=show_image,\n",
    "#     inputs=[\"image\", \"checkbox\", gr.Slider(0.01, 1, step=0.01, value=0.5)],\n",
    "#     outputs=[\"image\"],\n",
    "# )\n",
    "# demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-14T13:45:30.479241Z",
     "start_time": "2022-10-14T13:45:30.472424Z"
    }
   },
   "outputs": [],
   "source": [
    "# with gr.Blocks() as demo:\n",
    "#     output = gr.Image(label=\"Output Image\")\n",
    "\n",
    "# demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-14T13:49:07.273959Z",
     "start_time": "2022-10-14T13:49:07.271125Z"
    }
   },
   "outputs": [],
   "source": [
    "# demo = gr.Interface(\n",
    "#     fn=show_image,\n",
    "#     inputs=[\"image\", gr.Slider(0.01, 1, step=0.01, value=0.5)],\n",
    "#     outputs=[\"image\"]\n",
    "# )\n",
    "\n",
    "# demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-14T13:40:11.090748Z",
     "start_time": "2022-10-14T13:40:11.087938Z"
    }
   },
   "outputs": [],
   "source": [
    "# demo = gr.Blocks()\n",
    "\n",
    "# with demo:\n",
    "#     im = gr.Interface(show_image, gr.Image(), \"image\", tool='editor', allow_flagging=\"never\")\n",
    "# #     slider = gr.Slider(0.01, 1, step=0.01, value=0.5)\n",
    "# #     audio_file = gr.Audio(type=\"filepath\")\n",
    "# #     text = gr.Textbox()\n",
    "\n",
    "# demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-05T16:55:20.560122Z",
     "start_time": "2022-10-05T16:55:20.534387Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Create widget items.\n",
    "# btn_upload = widgets.FileUpload(multiple=False)\n",
    "# output1 = widgets.Output()\n",
    "# output1.layout.height = '200px'\n",
    "# scale_factor_slider = widgets.FloatSlider(value=0.5, min=0.05, max=1, step=0.01, description=\"Scale Factor\")\n",
    "# output2 = widgets.Output()\n",
    "# output2.layout.height = '30px'\n",
    "# output3 = widgets.Output()\n",
    "# predicted_label = widgets.Label()\n",
    "# btn_run = widgets.Button(description='Count People')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-05T16:55:20.563720Z",
     "start_time": "2022-10-05T16:55:20.561309Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Variables\n",
    "# input_image = None\n",
    "# x_res = None\n",
    "# y_res = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-05T16:55:59.023750Z",
     "start_time": "2022-10-05T16:55:59.012585Z"
    }
   },
   "outputs": [],
   "source": [
    "# def get_scale_factor(x_res, y_res):\n",
    "#     min_res = min(x_res, y_res)\n",
    "#     max_res = max(x_res, y_res)\n",
    "#     ideal_min = 1200\n",
    "#     if min_res < ideal_min:\n",
    "#         factor = 1\n",
    "#     if min_res > ideal_min:\n",
    "#         factor = round(ideal_min / min_res, 2) + 0.01\n",
    "#         if factor > 1:\n",
    "#             factor = 1\n",
    "#     return factor\n",
    "\n",
    "\n",
    "# def get_crops_amount(input_image, scale_factor):\n",
    "#     image = rescale_image(input_image, scale_factor)\n",
    "#     img_stack, pred_stack, img_h, img_w = prepare_loaded_image(image)\n",
    "#     n_crops = len(img_stack)\n",
    "#     return n_crops\n",
    "\n",
    "\n",
    "# # Give the user some feedback and change the downscale factor, when a new image is uploaded.\n",
    "# def on_data_change(data):\n",
    "#     output1.clear_output()\n",
    "#     with output1:\n",
    "#         global input_image, x_res, y_res\n",
    "#         input_image = PILImage.create(io.BytesIO(btn_upload.data[-1]))\n",
    "#         y_res, x_res = input_image.shape\n",
    "#         display(input_image.to_thumb(150,150))\n",
    "#         if x_res<224 or y_res<224:\n",
    "#             print(f\"Image is too small to process, only {x_res}x{y_res} pixels.\")\n",
    "#             print(\"Please upload another image with at least 224 pixels in both dimensions.\")\n",
    "#         else:\n",
    "#             scale_factor = get_scale_factor(x_res, y_res)\n",
    "#             scale_factor_slider.value = scale_factor\n",
    "#             print(f\"Original image size: {x_res}x{y_res}.\")\n",
    "#             if scale_factor_slider.value < 1:\n",
    "#                 downscale_x = int(x_res * scale_factor_slider.value)\n",
    "#                 downscale_y = int(y_res * scale_factor_slider.value)\n",
    "#                 print(f\"Proposing to downscale to {downscale_x}x{downscale_y} (scale factor: {scale_factor_slider.value}).\")\n",
    "#             else:\n",
    "#                 downscale_x = x_res\n",
    "#                 downscale_y = y_res\n",
    "#                 print(\"No downscaling proposed.\")\n",
    "# #             n_crops = get_crops_amount(input_image, scale_factor_slider.value)\n",
    "# #             print(f\"Processing {n_crops} crops.\")\n",
    "# #             approximate_n_crops = round((downscale_x*downscale_y) / 30000)  # High estimate for larger images.\n",
    "# #             print(f\"Approximate {approximate_n_crops} crops.\")\n",
    "    \n",
    "# btn_upload.observe(on_data_change, names=['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-05T16:55:59.156012Z",
     "start_time": "2022-10-05T16:55:59.143655Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Estimate processing time when changing downscale slider.\n",
    "\n",
    "# def on_slider_change(value):\n",
    "#     output2.clear_output()\n",
    "#     with output2:\n",
    "#         downscale_x = int(x_res * scale_factor_slider.value)\n",
    "#         downscale_y = int(y_res * scale_factor_slider.value)\n",
    "#         approximate_n_crops = round((downscale_x*downscale_y) / 30000) # High estimate for larger images.\n",
    "#         approximate_processing_time = int(approximate_n_crops / 6.5)  # on fast server.\n",
    "# #         approximate_processing_time = int(approximate_n_crops * 4.5)  # on slow server.\n",
    "# #         print(f\"Approximate number of crops: {approximate_n_crops}\")\n",
    "#         print(f\"Approximate processing time: {approximate_processing_time} seconds\")\n",
    "        \n",
    "# scale_factor_slider.observe(on_slider_change, 'value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-05T16:55:59.287780Z",
     "start_time": "2022-10-05T16:55:59.275180Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Define click interaction.\n",
    "# def on_click_classify(change):\n",
    "# #     input_image = PILImage.create(io.BytesIO(btn_upload.data[-1]))\n",
    "#     output3.clear_output()\n",
    "#     with output3:\n",
    "        \n",
    "#         # Scale image\n",
    "#         scale_factor = scale_factor_slider.value\n",
    "# #         print(f\"Scale factor used for downscaling image: {scale_factor}\")\n",
    "#         image = rescale_image(input_image, scale_factor)\n",
    "        \n",
    "#         # Show input image\n",
    "#         #display(img.to_thumb(128,128))\n",
    "# #         plt.figure(figsize=(1920/200, 1080/200), dpi=200)\n",
    "# #         plt.figure(figsize=(1440/200, 810/200), dpi=200)\n",
    "# #         plt.imshow(image, cmap=cm.jet)\n",
    "# #         plt.title(f'Input image for the network')\n",
    "# #         plt.show()\n",
    "        \n",
    "#         # Process image\n",
    "#         img_stack, pred_stack, img_h, img_w = prepare_loaded_image(image)\n",
    "#         den, pred_cnt = process_image(img_stack, pred_stack, img_h, img_w)\n",
    "        \n",
    "#         # Show model prediction\n",
    "# #         plt.figure(figsize=(1920/200, 1080/200), dpi=200)\n",
    "#         plt.figure(figsize=(1440/200, 810/200), dpi=200)\n",
    "#         plt.imshow(den, cmap=cm.jet)\n",
    "#         plt.title(f'Predicted count: {pred_cnt:.3f}')\n",
    "#         plt.show()\n",
    "        \n",
    "#         # Show overlay\n",
    "#         show_overlay(image, den, pred_cnt)\n",
    "\n",
    "# btn_run.on_click(on_click_classify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-05T16:55:59.637693Z",
     "start_time": "2022-10-05T16:55:59.580804Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14d022f519df4159969e4ae7d356858d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=\"<b><font color='DarkRed'>Choose an image to upload:</b>\"), FileUpload(value={'red.j…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Render app.\n",
    "# # VBox([widgets.Label(\"Choose and image to upload\"),\n",
    "# VBox([widgets.HTML(value = f\"<b><font color='DarkRed'>Choose an image to upload:</b>\"),\n",
    "#       btn_upload, output1, scale_factor_slider, output2, btn_run, output3, predicted_label])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
