{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Public Eye - Sample Version\n",
    "\n",
    "**Upload a picture of a crowd and let Public Eye count/estimate the amount of people!**\n",
    "\n",
    "This website is a slow and minimal sample version of the full Public Eye system.\n",
    "\n",
    "For more information about Public Eye, please get in touch at ADD_EMAIL_HERE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-22T18:32:55.518132Z",
     "start_time": "2022-08-22T18:32:55.509928Z"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-22T18:32:57.336790Z",
     "start_time": "2022-08-22T18:32:55.520707Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "\n",
    "import models.ViCCT_models\n",
    "import models.Swin_ViCCT_models\n",
    "from timm.models import create_model\n",
    "\n",
    "from datasets.dataset_utils import img_equal_split, img_equal_unsplit\n",
    "import torchvision.transforms as standard_transforms\n",
    "\n",
    "from fastai.vision.widgets import *\n",
    "from fastbook import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-22T18:32:57.342209Z",
     "start_time": "2022-08-22T18:32:57.338304Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set some parameters\n",
    "# Several parameters need to be defined to run this notebook.\n",
    "\n",
    "# First, which model will we use?\n",
    "# The generic ViCCT version 1 model is specified with 'ViCCT_base'. \n",
    "# The version 2 ViCCT model, which has Swin as its base, is specified with 'Swin_ViCCT_large_22k'.\n",
    "# model_name = 'ViCCT_base'\n",
    "model_name = 'Swin_ViCCT_large_22k'\n",
    "\n",
    "# The model is trained to perform crowd counting. We specify here where the weights of this trained model is located.\n",
    "# weights_path = 'models/trained_models/ViCCT_base_generic_1300_epochs.pth'\n",
    "weights_path = 'models/trained_models/Swin_ViCCT_large_22k_generic_1600_epochs.pth'\n",
    "\n",
    "# Some images are of extremely large resolution. When the heads in images occupy many (e.g. something like 100 x 100 \n",
    "# pixels each) pixels, the model is unable to make pretty predictions. One way to overcome this issue is to scale the image\n",
    "# by some factor. This factory is specified here. A factor of 1. means no scaling is performed.\n",
    "# scale_factor = 1.\n",
    "\n",
    "# We might want to save the predictions. Set 'save_results' to true if you want to save the prediction. Three figures are saved\n",
    "# 1) The input image for the network. 2) The network's prediction. 3) The predictions overlayed with the input.\n",
    "save_results = True\n",
    "\n",
    "# Lastly, do we use cuda? If you have cuda, it's advised to use it.\n",
    "use_cuda = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-22T18:32:59.510998Z",
     "start_time": "2022-08-22T18:32:57.345073Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jongstra/Projects/eagle_eye_documenten/ViCCT/venv/lib/python3.10/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2895.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "def load_model(model_name, weights_path, use_cuda):\n",
    "    \"\"\" Creates the model and initialised it with the weights specified. \"\"\"\n",
    "    \n",
    "    model = create_model(  # From the timm library. This function created the model specific architecture.\n",
    "    model_name,\n",
    "    init_path=weights_path,\n",
    "    pretrained_cc=True,\n",
    "    drop_rate=None if 'Swin' in model_name else 0.,  # Dropout\n",
    "\n",
    "    # Bamboozled by Facebook. This isn't drop_path_rate, but rather 'drop_connect'.\n",
    "    # I'm not yet sure what it is for the Swin version\n",
    "    drop_path_rate=None if 'Swin' in model_name else 0.,\n",
    "    drop_block_rate=None,  # Drops our entire Transformer blocks I think? Not used for ViCCT.\n",
    "    )\n",
    "\n",
    "    if use_cuda:\n",
    "        model = model.cuda()  # Place model on GPU\n",
    "    \n",
    "    model = model.eval()\n",
    "    \n",
    "    return model\n",
    "\n",
    "# print('Loading the model...')\n",
    "model = load_model(model_name, weights_path, use_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-22T18:32:59.528067Z",
     "start_time": "2022-08-22T18:32:59.514076Z"
    }
   },
   "outputs": [],
   "source": [
    "# Main Functions\n",
    "\n",
    "\n",
    "# Only for hardcore users. No need to modify these.\n",
    "mean_std = ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # Mean and std.dev. of ImageNet\n",
    "overlap = 32  # We ensure crops have at least this many pixels of overlap.\n",
    "ignore_buffer = 16  # When reconsturting the whole density map, ignore this many pixels on crop prediction borders.\n",
    "\n",
    "train_img_transform = standard_transforms.Compose([\n",
    "    standard_transforms.ToTensor(),\n",
    "    standard_transforms.Normalize(*mean_std)\n",
    "])\n",
    "\n",
    "\n",
    "def rescale_image(img, scale_factor):\n",
    "    \n",
    "    # Get image dimensions\n",
    "    img_w, img_h = img.size\n",
    "    \n",
    "    # Rescale image\n",
    "    if scale_factor != 1.:\n",
    "        new_w, new_h = round(img_w * scale_factor), round(img_h * scale_factor)\n",
    "        img = img.resize((new_w, new_h))\n",
    "    \n",
    "    return img\n",
    "\n",
    "\n",
    "def prepare_loaded_image(img):\n",
    "    \n",
    "    # Get image dimensions\n",
    "    img_w, img_h = img.size\n",
    "    \n",
    "    # Before we make the prediction, we normalise the image and split it up into crops\n",
    "    img = train_img_transform(img)\n",
    "    img_stack = img_equal_split(img, 224, overlap)  # Split the image ensuring a minimum of 'overlap' of overlap between crops.\n",
    "\n",
    "    if use_cuda:\n",
    "        img_stack = img_stack.cuda()  # Place image stack on GPU        \n",
    "\n",
    "    # This is the placeholder where we store the model predictions.\n",
    "    pred_stack = torch.zeros(img_stack.shape[0], 1, 224, 224)\n",
    "    \n",
    "    return img_stack, pred_stack, img_h, img_w\n",
    "\n",
    "\n",
    "def process_image(img_stack, pred_stack, img_h, img_w):\n",
    "    if not use_cuda and img_stack.shape[0] > 100:  # If on CPU and more than 100 image crops.\n",
    "        print('\\033[93m'\n",
    "              'WARNING: you are making a prediction on a very large image. This might take a long time! '\n",
    "              'You may want to use a lower \"Scale Factor\" value for faster processing. '\n",
    "              'You can stop a running process by pressing F5.'\n",
    "              '\\033[0m')\n",
    "\n",
    "    with torch.no_grad():  # Dont make gradients\n",
    "        print(f\"Processing {len(img_stack)} image parts.\")\n",
    "        for idx, img_crop in enumerate(tqdm(img_stack)):  # For each image crop\n",
    "            pred_stack[idx] = model.forward(img_crop.unsqueeze(0)).cpu()  # Make prediction.\n",
    "    print('Done!')\n",
    "\n",
    "\n",
    "    # Unsplit the perdiction crops to get the entire density map of the image.\n",
    "    den = img_equal_unsplit(pred_stack, overlap, ignore_buffer, img_h, img_w, 1)\n",
    "    den = den.squeeze()  # Remove the channel dimension\n",
    "\n",
    "    # Compute the perdicted count, which is the sum of the entire density map. Note that the model is trained with density maps\n",
    "    # scaled by a factor of 3000 (See sec 5.2 of my thesis for why: https://scripties.uba.uva.nl/search?id=723178). In short,\n",
    "    # This works :)\n",
    "    pred_cnt = den.sum() / 3000\n",
    "    \n",
    "    return den, pred_cnt\n",
    "\n",
    "\n",
    "def show_overlay(input_image, den, pred_cnt):\n",
    "    img_heat = np.array(input_image)\n",
    "    den_heat = den.clone().numpy()\n",
    "\n",
    "    den_heat = den_heat / 3000  # Scale values to original domain\n",
    "    den_heat[den_heat < 0] = 0  # Remove negative values\n",
    "    den_heat = den_heat / den_heat.max() # Normalise between 0 and 1\n",
    "\n",
    "    den_heat **= 0.5  # Reduce large values, increase small values\n",
    "    den_heat *= 255  # Values from 0 to 255 now\n",
    "    den_heat[den_heat < 50] = 0  # Threshold of 50\n",
    "\n",
    "    img_heat[:, :, 0][den_heat > 0] = img_heat[:, :, 0][den_heat > 0] / 2\n",
    "    img_heat[:, :, 1][den_heat > 0] = img_heat[:, :, 1][den_heat > 0] / 2\n",
    "    img_heat[:, :, 2][den_heat > 0] = den_heat[den_heat > 0]\n",
    "\n",
    "\n",
    "#     plt.figure(figsize=(1920/200, 1080/200), dpi=200)\n",
    "    plt.figure(figsize=(1440/200, 810/200), dpi=200)\n",
    "    plt.imshow(img_heat)\n",
    "    plt.title(f'Predicted count: {pred_cnt:.3f}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-22T18:32:59.552855Z",
     "start_time": "2022-08-22T18:32:59.529459Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create widget items.\n",
    "btn_upload = widgets.FileUpload()\n",
    "scale_factor_slider = widgets.FloatSlider(value=0.5, min=0.05, max=1, step=0.05, description=\"Scale Factor\")\n",
    "out_pl = widgets.Output()\n",
    "lbl_pred = widgets.Label()\n",
    "btn_run = widgets.Button(description='Count People')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-22T18:32:59.559796Z",
     "start_time": "2022-08-22T18:32:59.554476Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define click interaction.\n",
    "def on_click_classify(change):\n",
    "    input_image = PILImage.create(io.BytesIO(btn_upload.value[-1].content))\n",
    "    out_pl.clear_output()\n",
    "    with out_pl:\n",
    "        \n",
    "        # Scale image\n",
    "        scale_factor = scale_factor_slider.value\n",
    "        print(f\"Scale factor used for downscaling image: {scale_factor}\")\n",
    "        image = rescale_image(input_image, scale_factor)\n",
    "        \n",
    "        # Show input image\n",
    "        #display(img.to_thumb(128,128))\n",
    "#         plt.figure(figsize=(1920/200, 1080/200), dpi=200)\n",
    "        plt.figure(figsize=(1440/200, 810/200), dpi=200)\n",
    "        plt.imshow(image, cmap=cm.jet)\n",
    "        plt.title(f'Input image for the network')\n",
    "        plt.show()\n",
    "        \n",
    "        # Process image\n",
    "        img_stack, pred_stack, img_h, img_w = prepare_loaded_image(image)\n",
    "        den, pred_cnt = process_image(img_stack, pred_stack, img_h, img_w)\n",
    "        \n",
    "        # Show model prediction\n",
    "#         plt.figure(figsize=(1920/200, 1080/200), dpi=200)\n",
    "        plt.figure(figsize=(1440/200, 810/200), dpi=200)\n",
    "        plt.imshow(den, cmap=cm.jet)\n",
    "        plt.title(f'Predicted count: {pred_cnt:.3f}')\n",
    "        plt.show()\n",
    "        \n",
    "        # Show overlay\n",
    "        show_overlay(image, den, pred_cnt)\n",
    "\n",
    "btn_run.on_click(on_click_classify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-22T18:32:59.579137Z",
     "start_time": "2022-08-22T18:32:59.561576Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0f92202f1ab4b24a58e76568678fc9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=\"<b><font color='DarkRed'>Choose an image to upload:</b>\"), FileUpload(value=(), desâ€¦"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Render app.\n",
    "# VBox([widgets.Label(\"Choose and image to upload\"),\n",
    "VBox([widgets.HTML(value = f\"<b><font color='DarkRed'>Choose an image to upload:</b>\"),\n",
    "      btn_upload, scale_factor_slider, btn_run, out_pl, lbl_pred])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
