{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amsterdam Crowd Counter\n",
    "\n",
    "**Upload a picture of a crowd and let our Crowd Counter count/estimate the amount of people!**\n",
    "\n",
    "For more information, please get in touch at [crowdcounter@amsterdam.nl](mailto:crowdcounter@amsterdam.nl)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-18T15:31:02.115575Z",
     "start_time": "2022-10-18T15:31:02.089564Z"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-18T16:02:11.023564Z",
     "start_time": "2022-10-18T16:02:11.018902Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import math\n",
    "import datetime\n",
    "# from tqdm.notebook import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "\n",
    "import models.ViCCT_models\n",
    "import models.Swin_ViCCT_models\n",
    "from timm.models import create_model\n",
    "\n",
    "from datasets.dataset_utils import img_equal_split, img_equal_unsplit\n",
    "import torchvision.transforms as standard_transforms\n",
    "\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-18T15:31:05.070475Z",
     "start_time": "2022-10-18T15:31:05.064445Z"
    }
   },
   "outputs": [],
   "source": [
    "# Explains some parameters.\n",
    "\n",
    "# First, which model will we use?\n",
    "# The generic ViCCT version 1 model is specified with 'ViCCT_base'. \n",
    "# The version 2 ViCCT model, which has Swin as its base, is specified with 'Swin_ViCCT_large_22k'.\n",
    "# model_name = 'ViCCT_base'\n",
    "# model_name = 'Swin_ViCCT_large_22k'\n",
    "\n",
    "# The model is trained to perform crowd counting. We specify here where the weights of this trained model is located.\n",
    "# weights_path = 'models/trained_models/ViCCT_base_generic_1300_epochs.pth'\n",
    "# weights_path = 'models/trained_models/Swin_ViCCT_large_22k_generic_1600_epochs.pth'\n",
    "\n",
    "# Some images are of extremely large resolution. When the heads in images occupy many (e.g. something like 100 x 100 \n",
    "# pixels each) pixels, the model is unable to make pretty predictions. One way to overcome this issue is to scale the image\n",
    "# by some factor. This factory is specified here. A factor of 1. means no scaling is performed.\n",
    "# scale_factor = 1.\n",
    "\n",
    "# Lastly, do we use cuda? If you have cuda, it's advised to use it.\n",
    "# use_cuda = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-18T15:31:05.077652Z",
     "start_time": "2022-10-18T15:31:05.072757Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set some global variables. Only for hardcore users, no need to modify these.\n",
    "\n",
    "mean_std = ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # Mean and std.dev. of ImageNet\n",
    "overlap = 32  # We ensure crops have at least this many pixels of overlap.\n",
    "ignore_buffer = 16  # When reconstructing the whole density map, ignore this many pixels on crop prediction borders.\n",
    "\n",
    "train_img_transform = standard_transforms.Compose([\n",
    "    standard_transforms.ToTensor(),\n",
    "    standard_transforms.Normalize(*mean_std)\n",
    "])\n",
    "\n",
    "model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-18T15:31:05.082614Z",
     "start_time": "2022-10-18T15:31:05.079177Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_model(model_name='Swin_ViCCT_large_22k',\n",
    "               weights_path='models/trained_models/Swin_ViCCT_large_22k_generic_1600_epochs.pth',\n",
    "               use_cuda=\"True\"):\n",
    "    \"\"\" Creates the ViCCT model and initialises it with the specified weights. \"\"\"\n",
    "    \n",
    "    model = create_model(  # From the timm library. This function created the model specific architecture.\n",
    "    model_name,\n",
    "    init_path=weights_path,\n",
    "    pretrained_cc=True,\n",
    "    drop_rate=None if 'Swin' in model_name else 0.,  # Dropout\n",
    "\n",
    "    # Bamboozled by Facebook. This isn't drop_path_rate, but rather 'drop_connect'.\n",
    "    # I'm not yet sure what it is for the Swin version\n",
    "    drop_path_rate=None if 'Swin' in model_name else 0.,\n",
    "    drop_block_rate=None,  # Drops our entire Transformer blocks I think? Not used for ViCCT.\n",
    "    )\n",
    "\n",
    "    if use_cuda:\n",
    "        model = model.cuda()  # Place model on GPU\n",
    "    \n",
    "    model = model.eval()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-18T15:31:05.086543Z",
     "start_time": "2022-10-18T15:31:05.083758Z"
    }
   },
   "outputs": [],
   "source": [
    "def fig2img(fig):\n",
    "    \"\"\"Convert a Matplotlib figure to a PIL Image and return it\"\"\"\n",
    "    buf = io.BytesIO()\n",
    "    fig.savefig(buf, format='png', bbox_inches='tight')\n",
    "    buf.seek(0)\n",
    "    img = Image.open(buf)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-18T15:31:05.091043Z",
     "start_time": "2022-10-18T15:31:05.087632Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_density_map_image(den, pred_cnt):\n",
    "    \"\"\"Create a density map image using the density map.\"\"\"\n",
    "    \n",
    "    fig = plt.figure(figsize=(1440/100, 810/100), dpi=100)\n",
    "    plt.title(f'Predicted count: {pred_cnt:.1f}')\n",
    "    plt.imshow(den, cmap=cm.jet)\n",
    "    ax = plt.gca()\n",
    "    ax.axes.xaxis.set_visible(False)\n",
    "    ax.axes.yaxis.set_visible(False)\n",
    "    den_im = fig2img(fig)\n",
    "    \n",
    "    # Clean up memory.\n",
    "    fig.clear()\n",
    "    plt.close(fig)\n",
    "    \n",
    "    return den_im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-18T15:31:05.097726Z",
     "start_time": "2022-10-18T15:31:05.092209Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_overlay_image(input_image, den, pred_cnt):\n",
    "    \"\"\"Use an image and its generated density map + prediction count to create & return an overlayed image.\"\"\"\n",
    "    \n",
    "    img_heat = np.array(input_image)\n",
    "    den_heat = den.clone().numpy()\n",
    "\n",
    "    den_heat = den_heat / 3000  # Scale values to original domain\n",
    "    den_heat[den_heat < 0] = 0  # Remove negative values\n",
    "    den_heat = den_heat / den_heat.max() # Normalise between 0 and 1\n",
    "\n",
    "    den_heat **= 0.5  # Reduce large values, increase small values\n",
    "    den_heat *= 255  # Values from 0 to 255 now\n",
    "    den_heat[den_heat < 50] = 0  # Threshold of 50\n",
    "\n",
    "    img_heat[:, :, 0][den_heat > 0] = img_heat[:, :, 0][den_heat > 0] / 2\n",
    "    img_heat[:, :, 1][den_heat > 0] = img_heat[:, :, 1][den_heat > 0] / 2\n",
    "    img_heat[:, :, 2][den_heat > 0] = den_heat[den_heat > 0]\n",
    "\n",
    "    fig = plt.figure(figsize=(1440/100, 810/100), dpi=100)\n",
    "    plt.title(f'Predicted count: {pred_cnt:.1f}')\n",
    "    plt.imshow(img_heat, cmap=cm.jet)\n",
    "        \n",
    "    ax = plt.gca()\n",
    "    ax.axes.xaxis.set_visible(False)\n",
    "    ax.axes.yaxis.set_visible(False)\n",
    "    overlay_im = fig2img(fig)\n",
    "    \n",
    "    # Clean up memory.\n",
    "    fig.clear()\n",
    "    plt.close(fig)\n",
    "    \n",
    "    return overlay_im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-18T15:31:05.103695Z",
     "start_time": "2022-10-18T15:31:05.099304Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_image(img_stack, pred_stack, img_h, img_w, use_cuda=True):\n",
    "    \"\"\"Process a prepared image (using its prepared elements) with the ViCCT model.\"\"\"\n",
    "    \n",
    "    if not use_cuda and img_stack.shape[0] > 100:  # If on CPU and more than 100 image crops.\n",
    "        print('\\033[93m'\n",
    "              'WARNING: you are making a prediction on a very large image. This might take a long time! '\n",
    "              'You may want to use a lower \"Scale Factor\" value for faster processing. '\n",
    "              'You can stop a running process by pressing F5.'\n",
    "              '\\033[0m')\n",
    "\n",
    "    with torch.no_grad():  # Dont make gradients\n",
    "        print(f\"Processing {len(img_stack)} image parts.\")\n",
    "        for idx, img_crop in enumerate(img_stack):  # For each image crop\n",
    "            pred_stack[idx] = model.forward(img_crop.unsqueeze(0)).cpu()  # Make prediction.\n",
    "    print('Done!')\n",
    "\n",
    "    # Unsplit the perdiction crops to get the entire density map of the image.\n",
    "    den = img_equal_unsplit(pred_stack, overlap, ignore_buffer, img_h, img_w, 1)\n",
    "    den = den.squeeze()  # Remove the channel dimension\n",
    "\n",
    "    # Compute the perdicted count, which is the sum of the entire density map. Note that the model is trained with density maps\n",
    "    # scaled by a factor of 3000 (See sec 5.2 of my thesis for why: https://scripties.uba.uva.nl/search?id=723178). In short,\n",
    "    # This works :)\n",
    "    pred_cnt = den.sum() / 3000\n",
    "    \n",
    "    return den, pred_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-18T15:31:05.110753Z",
     "start_time": "2022-10-18T15:31:05.106521Z"
    }
   },
   "outputs": [],
   "source": [
    "def prepare_loaded_image(img, use_cuda=True):\n",
    "    \"\"\"Prepare an image for processing with the ViCCT model.\"\"\"\n",
    "    \n",
    "    # Get image dimensions\n",
    "    img_w, img_h = img.size\n",
    "    \n",
    "    # Before we make the prediction, we normalise the image and split it up into crops\n",
    "    img = train_img_transform(img)\n",
    "    img_stack = img_equal_split(img, 224, overlap)  # Split the image ensuring a minimum of 'overlap' of overlap between crops.\n",
    "\n",
    "    if use_cuda:\n",
    "        img_stack = img_stack.cuda()  # Place image stack on GPU        \n",
    "\n",
    "    # This is the placeholder where we store the model predictions.\n",
    "    pred_stack = torch.zeros(img_stack.shape[0], 1, 224, 224)\n",
    "    \n",
    "    return img_stack, pred_stack, img_h, img_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-18T15:31:05.117479Z",
     "start_time": "2022-10-18T15:31:05.113243Z"
    }
   },
   "outputs": [],
   "source": [
    "def rescale_image(img, scale_factor):\n",
    "    \"\"\"Rescale and return an image based on the given scale factor.\"\"\"\n",
    "    \n",
    "    # Get image dimensions\n",
    "    img_w, img_h = img.size\n",
    "    \n",
    "    # Rescale image\n",
    "    if scale_factor != 1.:\n",
    "        new_w, new_h = round(img_w * scale_factor), round(img_h * scale_factor)\n",
    "        img = img.resize((new_w, new_h))\n",
    "    \n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-18T15:31:05.124530Z",
     "start_time": "2022-10-18T15:31:05.120213Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_scale_factor(image, ideal_min_res=2000):\n",
    "    \"\"\"Computes the scale factor for images.\"\"\"\n",
    "    \n",
    "    x_res, y_res = image.size\n",
    "\n",
    "    min_res = min(x_res, y_res)\n",
    "    if min_res < ideal_min_res:\n",
    "        factor = 1\n",
    "    if min_res > ideal_min_res:\n",
    "        factor = ideal_min_res / min_res\n",
    "        if factor > 1:\n",
    "            factor = 1\n",
    "    \n",
    "    return factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-18T15:31:05.134767Z",
     "start_time": "2022-10-18T15:31:05.127323Z"
    }
   },
   "outputs": [],
   "source": [
    "def normalize_image_orientation(img):\n",
    "    \"\"\"Modifies image to its normalized orientation/rotation using exif information. Returns normalized image.\"\"\"\n",
    "        \n",
    "    # Get image orientation from exit (return unchanged image if exif or rotation data is not available).\n",
    "    try:\n",
    "        exif = img.getexif()\n",
    "        orientation = dict(exif.items())[274]  # 274 is the exif key for image orientation.\n",
    "    except (KeyError) as e:\n",
    "        return img\n",
    "    \n",
    "    # Rotate image to normal orientation.\n",
    "    if orientation == 2:\n",
    "        img = img.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "    elif orientation == 3:\n",
    "        img = img.rotate(180)\n",
    "    elif orientation == 4:\n",
    "        img = img.rotate(180).transpose(Image.FLIP_LEFT_RIGHT)\n",
    "    elif orientation == 5:\n",
    "        img = img.rotate(-90, expand=True).transpose(Image.FLIP_LEFT_RIGHT)\n",
    "    elif orientation == 6:\n",
    "        img = img.rotate(-90, expand=True)\n",
    "    elif orientation == 7:\n",
    "        img = img.rotate(90, expand=True).transpose(Image.FLIP_LEFT_RIGHT)\n",
    "    elif orientation == 8:\n",
    "        img = img.rotate(90, expand=True)\n",
    "        \n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-18T15:45:05.860798Z",
     "start_time": "2022-10-18T15:45:05.854360Z"
    }
   },
   "outputs": [],
   "source": [
    "def count_people(image_input):\n",
    "    \"\"\"Count the amount of people in an image. Return the resulting density map image, overlay image, and count.\"\"\"\n",
    "    \n",
    "    t0 = datetime.datetime.now()\n",
    "    \n",
    "    # Normalize image orientation.\n",
    "    image_input = normalize_image_orientation(image_input)\n",
    "        \n",
    "    # Rescale image.\n",
    "    scale_factor = compute_scale_factor(image_input)\n",
    "    image = rescale_image(image_input, scale_factor)\n",
    "    \n",
    "    # Give the user an error for images with a too low resolution. (alternative: upscale)\n",
    "    w = image.width\n",
    "    h = image.height\n",
    "    if w < 224 or h < 224:\n",
    "        raise gr.Error(\"Image is too small, please provide a bigger image (244x244 or larger) and try again.\")\n",
    "        return None, None, 0\n",
    "    \n",
    "    # Prepare and process image (create prediction).\n",
    "    img_stack, pred_stack, img_h, img_w = prepare_loaded_image(image)\n",
    "    den, pred_cnt = process_image(img_stack, pred_stack, img_h, img_w)\n",
    "    \n",
    "    # Create density map image.\n",
    "    den_im = create_density_map_image(den, pred_cnt)\n",
    "    \n",
    "    # Create overlay image.\n",
    "    overlay_im = create_overlay_image(image, den, pred_cnt)\n",
    "    \n",
    "    # Log succesful counting.\n",
    "    t1 = datetime.datetime.now()\n",
    "    processing_time = (t1 -t0).total_seconds()\n",
    "    with open(\"log.txt\", \"a\") as myfile:\n",
    "        myfile.write(f\"{t1}; succesfully processed an image of size {w}*{h} (w*h) -after possible downscaling- in {processing_time} seconds.\\n\")\n",
    "\n",
    "    return den_im, overlay_im, round(float(pred_cnt),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-18T15:45:08.822550Z",
     "start_time": "2022-10-18T15:45:06.895263Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load the model.\n",
    "model = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-18T16:02:43.303290Z",
     "start_time": "2022-10-18T16:02:43.296448Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Launch the demo website.\n",
    "\n",
    "def launch_demo():\n",
    "    demo = gr.Blocks(title=\"Crowd Counter\")\n",
    "\n",
    "    with demo:\n",
    "\n",
    "        # Introduction.\n",
    "        gr.Markdown(\"# Amsterdam Crowd Counter\")\n",
    "        gr.Markdown(\"Upload an image & count people.\")\n",
    "\n",
    "        # Interactive elements.\n",
    "        image_input = gr.Image(type='pil')\n",
    "        count_button = gr.Button(\"Count People\")\n",
    "        count_result = gr.Number(label=\"People Count\", elem_id='count', visible=False)\n",
    "        with gr.Row():\n",
    "            image_output_overlay = gr.Image(elem_id='output_image')\n",
    "            image_output = gr.Image(elem_id='output_image')\n",
    "\n",
    "        # Interactions.\n",
    "        count_button.click(fn=count_people, inputs=image_input, outputs=[image_output, image_output_overlay, count_result])\n",
    "\n",
    "        # Explanation about this website/service.\n",
    "        gr.Markdown(\"\"\"Counting results are generated using an AI model called [ViCCT](https://github.com/jongstra/ViCCT).\n",
    "                       This model is trained using multiple annotated datasets with large amounts of crowds.\n",
    "                       The resulting model is only usable for counting people and estimating crowd densities,\n",
    "                       not for identifying individuals.\"\"\")\n",
    "        gr.Markdown(\"\"\"This service is in testing phase and is provided \"as-is\",\n",
    "                       without warranty of any kind, nor any guarantees about correctness of results.\n",
    "                       This service should never be used as a sole means of crowd size estimation,\n",
    "                       but is intended to be used for human-assisted solutions.\"\"\")\n",
    "        gr.Markdown(\"For questions/feedback, contact us at [crowdcounter@amsterdam.nl](mailto:crowdcounter@amsterdam.nl).\")\n",
    "\n",
    "    demo.launch(share=False)\n",
    "#     demo.launch(server_port=8800, share=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-18T16:02:49.719975Z",
     "start_time": "2022-10-18T16:02:43.698910Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7865\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7865/\" width=\"900\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    gr.close_all()  # Try to close any running Gradio processes, to free up ports.\n",
    "    global model\n",
    "    model = load_model()\n",
    "    launch_demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
