{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Public Eye - Sample Version\n",
    "\n",
    "**Upload a picture of a crowd and let Public Eye count/estimate the amount of people!**\n",
    "\n",
    "This website is a slow and minimal sample version of the full Public Eye system.\n",
    "\n",
    "For more information about Public Eye, please get in touch at [crowdcounter@amsterdam.nl](mailto:crowdcounter@amsterdam.nl)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-18T09:07:49.244079Z",
     "start_time": "2022-10-18T09:07:49.214090Z"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-18T09:07:55.319284Z",
     "start_time": "2022-10-18T09:07:49.686740Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "\n",
    "import models.ViCCT_models\n",
    "import models.Swin_ViCCT_models\n",
    "from timm.models import create_model\n",
    "\n",
    "from datasets.dataset_utils import img_equal_split, img_equal_unsplit\n",
    "import torchvision.transforms as standard_transforms\n",
    "\n",
    "from fastai.vision.widgets import *\n",
    "from fastbook import *\n",
    "\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-18T09:07:55.337544Z",
     "start_time": "2022-10-18T09:07:55.326055Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set some parameters\n",
    "# Several parameters need to be defined to run this notebook.\n",
    "\n",
    "# First, which model will we use?\n",
    "# The generic ViCCT version 1 model is specified with 'ViCCT_base'. \n",
    "# The version 2 ViCCT model, which has Swin as its base, is specified with 'Swin_ViCCT_large_22k'.\n",
    "# model_name = 'ViCCT_base'\n",
    "model_name = 'Swin_ViCCT_large_22k'\n",
    "\n",
    "# The model is trained to perform crowd counting. We specify here where the weights of this trained model is located.\n",
    "# weights_path = 'models/trained_models/ViCCT_base_generic_1300_epochs.pth'\n",
    "weights_path = 'models/trained_models/Swin_ViCCT_large_22k_generic_1600_epochs.pth'\n",
    "\n",
    "# Some images are of extremely large resolution. When the heads in images occupy many (e.g. something like 100 x 100 \n",
    "# pixels each) pixels, the model is unable to make pretty predictions. One way to overcome this issue is to scale the image\n",
    "# by some factor. This factory is specified here. A factor of 1. means no scaling is performed.\n",
    "# scale_factor = 1.\n",
    "\n",
    "# We might want to save the predictions. Set 'save_results' to true if you want to save the prediction. Three figures are saved\n",
    "# 1) The input image for the network. 2) The network's prediction. 3) The predictions overlayed with the input.\n",
    "save_results = True\n",
    "\n",
    "# Lastly, do we use cuda? If you have cuda, it's advised to use it.\n",
    "use_cuda = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-18T09:08:00.356810Z",
     "start_time": "2022-10-18T09:07:55.345710Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load model\n",
    "def load_model(model_name, weights_path, use_cuda):\n",
    "    \"\"\" Creates the model and initialised it with the weights specified. \"\"\"\n",
    "    \n",
    "    model = create_model(  # From the timm library. This function created the model specific architecture.\n",
    "    model_name,\n",
    "    init_path=weights_path,\n",
    "    pretrained_cc=True,\n",
    "    drop_rate=None if 'Swin' in model_name else 0.,  # Dropout\n",
    "\n",
    "    # Bamboozled by Facebook. This isn't drop_path_rate, but rather 'drop_connect'.\n",
    "    # I'm not yet sure what it is for the Swin version\n",
    "    drop_path_rate=None if 'Swin' in model_name else 0.,\n",
    "    drop_block_rate=None,  # Drops our entire Transformer blocks I think? Not used for ViCCT.\n",
    "    )\n",
    "\n",
    "    if use_cuda:\n",
    "        model = model.cuda()  # Place model on GPU\n",
    "    \n",
    "    model = model.eval()\n",
    "    \n",
    "    return model\n",
    "\n",
    "# print('Loading the model...')\n",
    "model = load_model(model_name, weights_path, use_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-18T09:08:00.367212Z",
     "start_time": "2022-10-18T09:08:00.359355Z"
    }
   },
   "outputs": [],
   "source": [
    "# Main Functions\n",
    "\n",
    "\n",
    "# Only for hardcore users. No need to modify these.\n",
    "mean_std = ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # Mean and std.dev. of ImageNet\n",
    "overlap = 32  # We ensure crops have at least this many pixels of overlap.\n",
    "ignore_buffer = 16  # When reconsturting the whole density map, ignore this many pixels on crop prediction borders.\n",
    "\n",
    "train_img_transform = standard_transforms.Compose([\n",
    "    standard_transforms.ToTensor(),\n",
    "    standard_transforms.Normalize(*mean_std)\n",
    "])\n",
    "\n",
    "\n",
    "def rescale_image(img, scale_factor):\n",
    "    # Get image dimensions\n",
    "    img_w, img_h = img.size\n",
    "    \n",
    "    # Rescale image\n",
    "    if scale_factor != 1.:\n",
    "        new_w, new_h = round(img_w * scale_factor), round(img_h * scale_factor)\n",
    "        img = img.resize((new_w, new_h))\n",
    "    \n",
    "    return img\n",
    "\n",
    "\n",
    "def prepare_loaded_image(img):\n",
    "    # Get image dimensions\n",
    "    img_w, img_h = img.size\n",
    "    \n",
    "    # Before we make the prediction, we normalise the image and split it up into crops\n",
    "    img = train_img_transform(img)\n",
    "    img_stack = img_equal_split(img, 224, overlap)  # Split the image ensuring a minimum of 'overlap' of overlap between crops.\n",
    "\n",
    "    if use_cuda:\n",
    "        img_stack = img_stack.cuda()  # Place image stack on GPU        \n",
    "\n",
    "    # This is the placeholder where we store the model predictions.\n",
    "    pred_stack = torch.zeros(img_stack.shape[0], 1, 224, 224)\n",
    "    \n",
    "    return img_stack, pred_stack, img_h, img_w\n",
    "\n",
    "\n",
    "def process_image(img_stack, pred_stack, img_h, img_w):\n",
    "    if not use_cuda and img_stack.shape[0] > 100:  # If on CPU and more than 100 image crops.\n",
    "        print('\\033[93m'\n",
    "              'WARNING: you are making a prediction on a very large image. This might take a long time! '\n",
    "              'You may want to use a lower \"Scale Factor\" value for faster processing. '\n",
    "              'You can stop a running process by pressing F5.'\n",
    "              '\\033[0m')\n",
    "\n",
    "    with torch.no_grad():  # Dont make gradients\n",
    "        print(f\"Processing {len(img_stack)} image parts.\")\n",
    "        for idx, img_crop in enumerate(tqdm(img_stack)):  # For each image crop\n",
    "            pred_stack[idx] = model.forward(img_crop.unsqueeze(0)).cpu()  # Make prediction.\n",
    "    print('Done!')\n",
    "\n",
    "\n",
    "    # Unsplit the perdiction crops to get the entire density map of the image.\n",
    "    den = img_equal_unsplit(pred_stack, overlap, ignore_buffer, img_h, img_w, 1)\n",
    "    den = den.squeeze()  # Remove the channel dimension\n",
    "\n",
    "    # Compute the perdicted count, which is the sum of the entire density map. Note that the model is trained with density maps\n",
    "    # scaled by a factor of 3000 (See sec 5.2 of my thesis for why: https://scripties.uba.uva.nl/search?id=723178). In short,\n",
    "    # This works :)\n",
    "    pred_cnt = den.sum() / 3000\n",
    "    \n",
    "    return den, pred_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-18T09:26:43.943308Z",
     "start_time": "2022-10-18T09:26:43.924324Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_overlay_image(input_image, den, pred_cnt):\n",
    "    img_heat = np.array(input_image)\n",
    "    den_heat = den.clone().numpy()\n",
    "\n",
    "    den_heat = den_heat / 3000  # Scale values to original domain\n",
    "    den_heat[den_heat < 0] = 0  # Remove negative values\n",
    "    den_heat = den_heat / den_heat.max() # Normalise between 0 and 1\n",
    "\n",
    "    den_heat **= 0.5  # Reduce large values, increase small values\n",
    "    den_heat *= 255  # Values from 0 to 255 now\n",
    "    den_heat[den_heat < 50] = 0  # Threshold of 50\n",
    "\n",
    "    img_heat[:, :, 0][den_heat > 0] = img_heat[:, :, 0][den_heat > 0] / 2\n",
    "    img_heat[:, :, 1][den_heat > 0] = img_heat[:, :, 1][den_heat > 0] / 2\n",
    "    img_heat[:, :, 2][den_heat > 0] = den_heat[den_heat > 0]\n",
    "\n",
    "    plt.figure(figsize=(1440/100, 810/100), dpi=100)\n",
    "    plt.title(f'Predicted count: {pred_cnt:.1f}')\n",
    "    plt.imshow(img_heat)\n",
    "        \n",
    "    ax = plt.gca()\n",
    "    ax.axes.xaxis.set_visible(False)\n",
    "    ax.axes.yaxis.set_visible(False)\n",
    "    overlay_im = io.BytesIO()\n",
    "    plt.savefig(overlay_im, format='png', bbox_inches='tight')\n",
    "    overlay_im = PILImage.create(overlay_im)\n",
    "    \n",
    "    return overlay_im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-18T09:13:05.258222Z",
     "start_time": "2022-10-18T09:13:05.254344Z"
    }
   },
   "outputs": [],
   "source": [
    "def normalize_image_orientation(img):\n",
    "    \n",
    "    # Get image orientation from exit (return unchanged image if rotation is not available).\n",
    "    try:\n",
    "        exif = img.getexif()\n",
    "        orientation = dict(exif.items())[274]  # 274 is the exif key for rotation.\n",
    "    except KeyError:\n",
    "        return img\n",
    "    \n",
    "    # Rotate image to normal rotation\n",
    "    if orientation == 2:\n",
    "        img = img.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "    elif orientation == 3:\n",
    "        img = img.rotate(180)\n",
    "    elif orientation == 4:\n",
    "        img = img.rotate(180).transpose(Image.FLIP_LEFT_RIGHT)\n",
    "    elif orientation == 5:\n",
    "        img = img.rotate(-90, expand=True).transpose(Image.FLIP_LEFT_RIGHT)\n",
    "    elif orientation == 6:\n",
    "        img = img.rotate(-90, expand=True)\n",
    "    elif orientation == 7:\n",
    "        img = img.rotate(90, expand=True).transpose(Image.FLIP_LEFT_RIGHT)\n",
    "    elif orientation == 8:\n",
    "        img = img.rotate(90, expand=True)\n",
    "        \n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-18T09:13:05.264633Z",
     "start_time": "2022-10-18T09:13:05.259443Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_scale_factor(image):\n",
    "    \n",
    "    x_res, y_res = image.size\n",
    "\n",
    "    min_res = min(x_res, y_res)\n",
    "    ideal_min = 2000\n",
    "    if min_res < ideal_min:\n",
    "        factor = 1\n",
    "    if min_res > ideal_min:\n",
    "        factor = round(ideal_min / min_res, 2) + 0.01\n",
    "        if factor > 1:\n",
    "            factor = 1\n",
    "    \n",
    "    return factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-18T09:29:28.648927Z",
     "start_time": "2022-10-18T09:29:27.477545Z"
    }
   },
   "outputs": [],
   "source": [
    "def count_people(image_input):\n",
    "    \n",
    "    # Normalize image orientation.\n",
    "    image_input = normalize_image_orientation(image_input)\n",
    "        \n",
    "    # Rescale image.\n",
    "    scale_factor = compute_scale_factor(image_input)\n",
    "    image = rescale_image(image_input, scale_factor)\n",
    "    \n",
    "    # Give the user an error for images with a too low resolution. (alternative: upscale)\n",
    "    w, h = image.shape\n",
    "    if w < 224 or h < 224:\n",
    "        raise gr.Error(\"Image is too small, please provide a bigger image (244x244 or larger) and try again.\")\n",
    "        return None, None, 0\n",
    "    \n",
    "    img_stack, pred_stack, img_h, img_w = prepare_loaded_image(image)\n",
    "    den, pred_cnt = process_image(img_stack, pred_stack, img_h, img_w)\n",
    "    \n",
    "    plt.figure(figsize=(1440/100, 810/100), dpi=100)\n",
    "    plt.title(f'Predicted count: {pred_cnt:.1f}')\n",
    "    plt.imshow(den, cmap=cm.jet)\n",
    "    ax = plt.gca()\n",
    "    ax.axes.xaxis.set_visible(False)\n",
    "    ax.axes.yaxis.set_visible(False)\n",
    "    den_im = io.BytesIO()\n",
    "    plt.savefig(den_im, format='png', bbox_inches='tight')\n",
    "    den_im = PILImage.create(den_im)\n",
    "    \n",
    "    overlay_im = create_overlay_image(image, den, pred_cnt)\n",
    "\n",
    "    return den_im, overlay_im, round(float(pred_cnt),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-18T09:18:45.236286Z",
     "start_time": "2022-10-18T09:18:41.885263Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# TODO:\n",
    "#\n",
    "# [X] Make count button work (use model, test GPU version as well)\n",
    "# [X] Put \"People Count\" result number above output image?\n",
    "# [X] Make image blocks higher in pixels (how??).\n",
    "# [X] Make \"Clear Image\" button work as expected. \n",
    "# [X] Build a catch when pressing the count_button if the (downscaled) image is of too low resolution.\n",
    "# [X] Fix dat beelden soms upside down staan na verwerking (rotation in exif niet goed opgepakt oid?)\n",
    "# [X] Default resolutie gebruiken (cap: 2000px) ipv scaling slider aanbieden?\n",
    "# [ ] Make code nicer/better maintainable.\n",
    "\n",
    "\n",
    "def reset():\n",
    "    return None, None, None, None\n",
    "    \n",
    "# demo = gr.Blocks(css=\"#output_image {height: 600px !important} #count {font-weight: bold;}\")\n",
    "\n",
    "demo = gr.Blocks(title=\"Crowd Counter\")\n",
    "\n",
    "with demo:\n",
    "\n",
    "    gr.Markdown(\"# Crowd Counter\")\n",
    "    gr.Markdown(\"Upload an image & count the people.\")\n",
    "    image_input = gr.Image(type='pil')\n",
    "    \n",
    "    count_button = gr.Button(\"Count People\")\n",
    "    count_result = gr.Number(label=\"People Count\", elem_id='count', visible=False)\n",
    "    with gr.Row():\n",
    "        image_output_overlay = gr.Image(elem_id='output_image')\n",
    "        image_output = gr.Image(elem_id='output_image')\n",
    "    \n",
    "    # Interactions\n",
    "    count_button.click(fn=count_people, inputs=image_input, outputs=[image_output, image_output_overlay, count_result])\n",
    "\n",
    "    # Footer\n",
    "    gr.Markdown(\"Contact us at: [crowdcounter@amsterdam.nl](mailto:crowdcounter@amsterdam.nl)\")\n",
    "    gr.Markdown(\"Created by Thomas Jongstra 2022, for the Municipality of Amsterdam.\")\n",
    "\n",
    "\n",
    "# demo.queue()\n",
    "demo.launch(share=False)\n",
    "# demo.launch(share=True)\n",
    "# demo.launch(server_port=8800, share=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
